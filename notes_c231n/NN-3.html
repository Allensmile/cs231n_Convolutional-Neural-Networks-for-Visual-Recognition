<!DOCTYPE html>
<!-- saved from url=(0042)http://cs231n.github.io/neural-networks-3/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>CS231n Convolutional Neural Networks for Visual Recognition</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Course materials and notes for Stanford class CS231n: Convolutional Neural Networks for Visual Recognition.">
    <link rel="canonical" href="http://cs231n.github.io/neural-networks-3/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="./NN-3_files/main.css">

    <!-- Google fonts -->
    <link href="./NN-3_files/css" rel="stylesheet" type="text/css">

    <!-- Google tracking -->
    <script async="" src="./NN-3_files/analytics.js"></script><script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46895817-2', 'auto');
      ga('send', 'pageview');

    </script>
    
<style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?rev=2.6.1') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?rev=2.6.1') format('woff'), url('http://cdn.mathjax.org/mathjax/2.6-latest/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?rev=2.6.1') format('opentype')}
</style></head>


    <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    <header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="http://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1></h1>
  </header>

  <article class="post-content">
  <p>Table of Contents:</p>

<ul>
  <li><a href="http://cs231n.github.io/neural-networks-3/#gradcheck">Gradient checks</a></li>
  <li><a href="http://cs231n.github.io/neural-networks-3/#sanitycheck">Sanity checks</a></li>
  <li><a href="http://cs231n.github.io/neural-networks-3/#baby">Babysitting the learning process</a>
    <ul>
      <li><a href="http://cs231n.github.io/neural-networks-3/#loss">Loss function</a></li>
      <li><a href="http://cs231n.github.io/neural-networks-3/#accuracy">Train/val accuracy</a></li>
      <li><a href="http://cs231n.github.io/neural-networks-3/#ratio">Weights:Updates ratio</a></li>
      <li><a href="http://cs231n.github.io/neural-networks-3/#distr">Activation/Gradient distributions per layer</a></li>
      <li><a href="http://cs231n.github.io/neural-networks-3/#vis">Visualization</a></li>
    </ul>
  </li>
  <li><a href="http://cs231n.github.io/neural-networks-3/#update">Parameter updates</a>
    <ul>
      <li><a href="http://cs231n.github.io/neural-networks-3/#sgd">First-order (SGD), momentum, Nesterov momentum</a></li>
      <li><a href="http://cs231n.github.io/neural-networks-3/#anneal">Annealing the learning rate</a></li>
      <li><a href="http://cs231n.github.io/neural-networks-3/#second">Second-order methods</a></li>
      <li><a href="http://cs231n.github.io/neural-networks-3/#ada">Per-parameter adaptive learning rates (Adagrad, RMSProp)</a></li>
    </ul>
  </li>
  <li><a href="http://cs231n.github.io/neural-networks-3/#hyper">Hyperparameter Optimization</a></li>
  <li><a href="http://cs231n.github.io/neural-networks-3/#eval">Evaluation</a>
    <ul>
      <li><a href="http://cs231n.github.io/neural-networks-3/#ensemble">Model Ensembles</a></li>
    </ul>
  </li>
  <li><a href="http://cs231n.github.io/neural-networks-3/#summary">Summary</a></li>
  <li><a href="http://cs231n.github.io/neural-networks-3/#add">Additional References</a></li>
</ul>

<h2 id="learning">Learning</h2>

<p>In the previous sections we’ve discussed the static parts of a Neural Networks: how we can set up the network connectivity, the data, and the loss function. This section is devoted to the dynamics, or in other words, the process of learning the parameters and finding good hyperparameters.</p>

<p><a name="gradcheck"></a></p>

<h3 id="gradient-checks">Gradient Checks</h3>

<p>In theory, performing a gradient check is as simple as comparing the analytic gradient to the numerical gradient. In practice, the process is much more involved and error prone. Here are some tips, tricks, and issues to watch out for:</p>

<p><strong>Use the centered formula</strong>. The formula you may have seen for the finite difference approximation when evaluating the numerical gradient looks as follows:</p>

<span class="MathJax_Preview" style="color: inherit;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mfrac&gt;&lt;mspace width=&quot;0.1in&quot; /&gt;&lt;mtext&gt;(bad, do not use)&lt;/mtext&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" role="math" style="width: 23.158em; display: inline-block;"><span style="display: inline-block; position: relative; width: 18.957em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(0.617em 1018.86em 3.128em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-2"><span class="mfrac" id="MathJax-Span-3"><span style="display: inline-block; position: relative; width: 2.564em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.076em 1002.31em 4.408em -999.997em); top: -4.711em; left: 50%; margin-left: -1.227em;"><span class="mrow" id="MathJax-Span-4"><span class="mi" id="MathJax-Span-5" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-6" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-7" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-8" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-9" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.128em 1001.03em 4.152em -999.997em); top: -3.327em; left: 50%; margin-left: -0.561em;"><span class="mrow" id="MathJax-Span-10"><span class="mi" id="MathJax-Span-11" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-12" style="font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(0.873em 1002.56em 1.232em -999.997em); top: -1.278em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top-width: 1.3px; border-top-style: solid; width: 2.564em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.078em;"></span></span></span></span><span class="mo" id="MathJax-Span-13" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="mfrac" id="MathJax-Span-14" style="padding-left: 0.259em;"><span style="display: inline-block; position: relative; width: 6.867em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.076em 1006.66em 4.408em -999.997em); top: -4.711em; left: 50%; margin-left: -3.379em;"><span class="mrow" id="MathJax-Span-15"><span class="mi" id="MathJax-Span-16" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-17" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-18" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-19" style="font-family: MathJax_Main; padding-left: 0.207em;">+</span><span class="mi" id="MathJax-Span-20" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-21" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-22" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="mi" id="MathJax-Span-23" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-24" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-25" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-26" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.128em 1000.57em 4.152em -999.997em); top: -3.327em; left: 50%; margin-left: -0.305em;"><span class="mi" id="MathJax-Span-27" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(0.873em 1006.87em 1.232em -999.997em); top: -1.278em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top-width: 1.3px; border-top-style: solid; width: 6.867em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.078em;"></span></span></span></span><span class="mspace" id="MathJax-Span-28" style="height: 0.003em; vertical-align: 0.003em; width: 0.515em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-29" style="font-family: MathJax_Main;">(bad, do not use)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 2.753em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mfrac><mrow><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mi>h</mi></mfrac><mspace width="0.1in"></mspace><mtext>(bad, do not use)</mtext></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-1">\frac{df(x)}{dx} = \frac{f(x + h) - f(x)}{h} \hspace{0.1in} \text{(bad, do not use)}</script>

<p>where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-30" role="math" style="width: 0.72em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.566em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1000.57em 2.359em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-31"><span class="mi" id="MathJax-Span-32" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">h</script> is a very small number, in practice approximately 1e-5 or so. In practice, it turns out that it is much better to use the <em>centered</em> difference formula of the form:</p>

<span class="MathJax_Preview" style="color: inherit;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mspace width=&quot;0.1in&quot; /&gt;&lt;mtext&gt;(use instead)&lt;/mtext&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-33" role="math" style="width: 23.158em; display: inline-block;"><span style="display: inline-block; position: relative; width: 18.957em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(0.617em 1018.86em 3.128em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-34"><span class="mfrac" id="MathJax-Span-35"><span style="display: inline-block; position: relative; width: 2.564em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.076em 1002.31em 4.408em -999.997em); top: -4.711em; left: 50%; margin-left: -1.227em;"><span class="mrow" id="MathJax-Span-36"><span class="mi" id="MathJax-Span-37" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-38" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-39" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-40" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-41" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.128em 1001.03em 4.152em -999.997em); top: -3.327em; left: 50%; margin-left: -0.561em;"><span class="mrow" id="MathJax-Span-42"><span class="mi" id="MathJax-Span-43" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-44" style="font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(0.873em 1002.56em 1.232em -999.997em); top: -1.278em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top-width: 1.3px; border-top-style: solid; width: 2.564em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.078em;"></span></span></span></span><span class="mo" id="MathJax-Span-45" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="mfrac" id="MathJax-Span-46" style="padding-left: 0.259em;"><span style="display: inline-block; position: relative; width: 8.66em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.076em 1008.46em 4.408em -999.997em); top: -4.711em; left: 50%; margin-left: -4.249em;"><span class="mrow" id="MathJax-Span-47"><span class="mi" id="MathJax-Span-48" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-49" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-50" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-51" style="font-family: MathJax_Main; padding-left: 0.207em;">+</span><span class="mi" id="MathJax-Span-52" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-53" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-54" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="mi" id="MathJax-Span-55" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-56" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-57" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-58" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="mi" id="MathJax-Span-59" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-60" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.128em 1001.08em 4.152em -999.997em); top: -3.327em; left: 50%; margin-left: -0.561em;"><span class="mrow" id="MathJax-Span-61"><span class="mn" id="MathJax-Span-62" style="font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-63" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(0.873em 1008.66em 1.232em -999.997em); top: -1.278em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top-width: 1.3px; border-top-style: solid; width: 8.66em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.078em;"></span></span></span></span><span class="mspace" id="MathJax-Span-64" style="height: 0.003em; vertical-align: 0.003em; width: 0.515em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-65" style="font-family: MathJax_Main;">(use instead)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.934em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 2.753em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mfrac><mrow><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>h</mi><mo stretchy="false">)</mo></mrow><mrow><mn>2</mn><mi>h</mi></mrow></mfrac><mspace width="0.1in"></mspace><mtext>(use instead)</mtext></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-3">\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in} \text{(use instead)}</script>

<p>This requires you to evaluate the loss function twice to check every single dimension of the gradient (so it is about 2 times as expensive), but the gradient approximation turns out to be much more precise. To see this, you can use Taylor expansion of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-66" role="math" style="width: 4.511em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.691em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.59em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-67"><span class="mi" id="MathJax-Span-68" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-69" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-70" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-71" style="font-family: MathJax_Main; padding-left: 0.207em;">+</span><span class="mi" id="MathJax-Span-72" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-73" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-4">f(x+h)</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-74" role="math" style="width: 4.511em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.691em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.59em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-75"><span class="mi" id="MathJax-Span-76" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-77" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-78" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-79" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="mi" id="MathJax-Span-80" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-81" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-5">f(x-h)</script> and verify that the first formula has an error on order of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-82" role="math" style="width: 2.564em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.103em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1002em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-83"><span class="mi" id="MathJax-Span-84" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-85" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-86" style="font-family: MathJax_Math-italic;">h</span><span class="mo" id="MathJax-Span-87" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-6">O(h)</script>, while the second formula only has error terms on order of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-88" role="math" style="width: 3.23em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.615em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.283em 1002.51em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-89"><span class="mi" id="MathJax-Span-90" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-91" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-92"><span style="display: inline-block; position: relative; width: 1.027em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.152em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-93" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -4.352em; left: 0.566em;"><span class="mn" id="MathJax-Span-94" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-95" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>h</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-7">O(h^2)</script> (i.e. it is a second order approximation).</p>

<p><strong>Use relative error for the comparison</strong>. What are the details of comparing the numerical gradient <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-96" role="math" style="width: 1.181em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1000.98em 2.564em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-97"><span class="msubsup" id="MathJax-Span-98"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.357em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-99" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.435em 1000.26em 4.101em -999.997em); top: -4.301em; left: 0.617em;"><span class="mo" id="MathJax-Span-100" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.537em 1000.51em 4.152em -999.997em); top: -3.84em; left: 0.515em;"><span class="mi" id="MathJax-Span-101" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>f</mi><mi>n</mi><mo>′</mo></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-8">f’_n</script> and analytic gradient <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-102" role="math" style="width: 1.181em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1000.98em 2.564em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-103"><span class="msubsup" id="MathJax-Span-104"><span style="display: inline-block; position: relative; width: 0.925em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.357em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-105" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.435em 1000.26em 4.101em -999.997em); top: -4.301em; left: 0.617em;"><span class="mo" id="MathJax-Span-106" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.537em 1000.46em 4.152em -999.997em); top: -3.84em; left: 0.515em;"><span class="mi" id="MathJax-Span-107" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>f</mi><mi>a</mi><mo>′</mo></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-9">f’_a</script>? That is, how do we know if the two are not compatible? You might be temped to keep track of the difference <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2223;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2223;&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-108" role="math" style="width: 4.46em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.64em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.54em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-109"><span class="mo" id="MathJax-Span-110" style="font-family: MathJax_Main;">∣</span><span class="msubsup" id="MathJax-Span-111"><span style="display: inline-block; position: relative; width: 0.925em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.357em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-112" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.435em 1000.26em 4.101em -999.997em); top: -4.301em; left: 0.617em;"><span class="mo" id="MathJax-Span-113" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.537em 1000.46em 4.152em -999.997em); top: -3.84em; left: 0.515em;"><span class="mi" id="MathJax-Span-114" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-115" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="msubsup" id="MathJax-Span-116" style="padding-left: 0.207em;"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.357em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-117" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.435em 1000.26em 4.101em -999.997em); top: -4.301em; left: 0.617em;"><span class="mo" id="MathJax-Span-118" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.537em 1000.51em 4.152em -999.997em); top: -3.84em; left: 0.515em;"><span class="mi" id="MathJax-Span-119" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-120" style="font-family: MathJax_Main;">∣</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">∣</mo><msubsup><mi>f</mi><mi>a</mi><mo>′</mo></msubsup><mo>−</mo><msubsup><mi>f</mi><mi>n</mi><mo>′</mo></msubsup><mo stretchy="false">∣</mo></math></span></span><script type="math/tex" id="MathJax-Element-10">\mid f’_a - f’_n \mid </script> or its square and define the gradient check as failed if that difference is above a threshold. However, this is problematic. For example, consider the case where their difference is 1e-4. This seems like a very appropriate difference if the two gradients are about 1.0, so we’d consider the two gradients to match. But if the gradients were both on order of 1e-5 or lower, then we’d consider 1e-4 to be a huge difference and likely a failure. Hence, it is always more appropriate to consider the <em>relative error</em>:</p>

<span class="MathJax_Preview" style="color: inherit;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2223;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2223;&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2223;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mo&gt;&amp;#x2223;&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;&amp;#x2223;&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msubsup&gt;&lt;mo&gt;&amp;#x2223;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-121" role="math" style="width: 9.224em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.533em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(0.566em 1007.53em 3.332em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-122"><span class="mfrac" id="MathJax-Span-123"><span style="display: inline-block; position: relative; width: 7.277em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.076em 1003.54em 4.408em -999.997em); top: -4.711em; left: 50%; margin-left: -1.842em;"><span class="mrow" id="MathJax-Span-124"><span class="mo" id="MathJax-Span-125" style="font-family: MathJax_Main;">∣</span><span class="msubsup" id="MathJax-Span-126"><span style="display: inline-block; position: relative; width: 0.925em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.357em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-127" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.435em 1000.26em 4.101em -999.997em); top: -4.301em; left: 0.617em;"><span class="mo" id="MathJax-Span-128" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.537em 1000.46em 4.152em -999.997em); top: -3.84em; left: 0.515em;"><span class="mi" id="MathJax-Span-129" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-130" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="msubsup" id="MathJax-Span-131" style="padding-left: 0.207em;"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.357em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-132" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.435em 1000.26em 4.101em -999.997em); top: -4.301em; left: 0.617em;"><span class="mo" id="MathJax-Span-133" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.537em 1000.51em 4.152em -999.997em); top: -3.84em; left: 0.515em;"><span class="mi" id="MathJax-Span-134" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-135" style="font-family: MathJax_Main;">∣</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.076em 1007.07em 4.408em -999.997em); top: -3.276em; left: 50%; margin-left: -3.584em;"><span class="mrow" id="MathJax-Span-136"><span class="mo" id="MathJax-Span-137" style="font-family: MathJax_Main;">max</span><span class="mo" id="MathJax-Span-138" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-139" style="font-family: MathJax_Main;">∣</span><span class="msubsup" id="MathJax-Span-140" style="padding-left: 0.259em;"><span style="display: inline-block; position: relative; width: 0.925em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.357em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-141" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.435em 1000.26em 4.101em -999.997em); top: -4.301em; left: 0.617em;"><span class="mo" id="MathJax-Span-142" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.537em 1000.46em 4.152em -999.997em); top: -3.84em; left: 0.515em;"><span class="mi" id="MathJax-Span-143" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-144" style="font-family: MathJax_Main; padding-left: 0.259em;">∣</span><span class="mo" id="MathJax-Span-145" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-146" style="font-family: MathJax_Main; padding-left: 0.156em;">∣</span><span class="msubsup" id="MathJax-Span-147" style="padding-left: 0.259em;"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.357em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-148" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.435em 1000.26em 4.101em -999.997em); top: -4.301em; left: 0.617em;"><span class="mo" id="MathJax-Span-149" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(3.537em 1000.51em 4.152em -999.997em); top: -3.84em; left: 0.515em;"><span class="mi" id="MathJax-Span-150" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-151" style="font-family: MathJax_Main; padding-left: 0.259em;">∣</span><span class="mo" id="MathJax-Span-152" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; clip: rect(0.873em 1007.28em 1.232em -999.997em); top: -1.278em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top-width: 1.3px; border-top-style: solid; width: 7.277em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.078em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 3.128em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mfrac><mrow><mo stretchy="false">∣</mo><msubsup><mi>f</mi><mi>a</mi><mo>′</mo></msubsup><mo>−</mo><msubsup><mi>f</mi><mi>n</mi><mo>′</mo></msubsup><mo stretchy="false">∣</mo></mrow><mrow><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><mo>∣</mo><msubsup><mi>f</mi><mi>a</mi><mo>′</mo></msubsup><mo>∣</mo><mo>,</mo><mo>∣</mo><msubsup><mi>f</mi><mi>n</mi><mo>′</mo></msubsup><mo>∣</mo><mo stretchy="false">)</mo></mrow></mfrac></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-11">\frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)}</script>

<p>which considers their ratio of the differences to the ratio of the absolute values of both gradients. Notice that normally the relative error formula only includes one of the two terms (either one), but I prefer to max (or add) both to make it symmetric and to prevent dividing by zero in the case where one of the two is zero (which can often happen, especially with ReLUs). However, one must explicitly keep track of the case where both are zero and pass the gradient check in that edge case. In practice:</p>

<ul>
  <li>relative error &gt; 1e-2 usually means the gradient is probably wrong</li>
  <li>1e-2 &gt; relative error &gt; 1e-4 should make you feel uncomfortable</li>
  <li>1e-4 &gt; relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high.</li>
  <li>1e-7 and less you should be happy.</li>
</ul>

<p>Also keep in mind that the deeper the network, the higher the relative errors will be. So if you are gradient checking the input data for a 10-layer network, a relative error of 1e-2 might be okay because the errors build up on the way. Conversely, an error of 1e-2 for a single differentiable function likely indicates incorrect gradient.</p>

<p><strong>Use double precision</strong>. A common pitfall is using single precision floating point to compute gradient check. It is often that case that you might get high relative errors (as high as 1e-2) even with a correct gradient implementation. In my experience I’ve sometimes seen my relative errors plummet from 1e-2 to 1e-8 by switching to double precision.</p>

<p><strong>Stick around active range of floating point</strong>. It’s a good idea to read through <a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">“What Every Computer Scientist Should Know About Floating-Point Arithmetic”</a>, as it may demystify your errors and enable you to write more careful code. For example, in neural nets it can be common to normalize the loss function over the batch. However, if your gradients per datapoint are very small, then <em>additionally</em> dividing them by the number of data points is starting to give very small numbers, which in turn will lead to more numerical issues. This is why I like to always print the raw numerical/analytic gradient, and make sure that the numbers you are comparing are not extremely small (e.g. roughly 1e-10 and smaller in absolute value is worrying). If they are you may want to temporarily scale your loss function up by a constant to bring them to a “nicer” range where floats are more dense - ideally on the order of 1.0, where your float exponent is 0.</p>

<p><strong>Kinks in the objective</strong>. One source of inaccuracy to be aware of during gradient checking is the problem of <em>kinks</em>. Kinks refer to non-differentiable parts of an objective function, introduced by functions such as ReLU (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-153" role="math" style="width: 5.228em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.255em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1004.15em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-154"><span class="mi" id="MathJax-Span-155" style="font-family: MathJax_Math-italic;">m</span><span class="mi" id="MathJax-Span-156" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-157" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-158" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-159" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-160" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-161" style="font-family: MathJax_Math-italic; padding-left: 0.156em;">x</span><span class="mo" id="MathJax-Span-162" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-12">max(0,x)</script>), or the SVM loss, Maxout neurons, etc. Consider gradient checking the ReLU function at <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-163" role="math" style="width: 5.023em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.101em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.437em 1004.05em 2.41em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-164"><span class="mi" id="MathJax-Span-165" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-166" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="mo" id="MathJax-Span-167" style="font-family: MathJax_Main; padding-left: 0.259em;">−</span><span class="mn" id="MathJax-Span-168" style="font-family: MathJax_Main;">1</span><span class="mi" id="MathJax-Span-169" style="font-family: MathJax_Math-italic;">e</span><span class="mn" id="MathJax-Span-170" style="font-family: MathJax_Main;">6</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>=</mo><mo>−</mo><mn>1</mn><mi>e</mi><mn>6</mn></math></span></span><script type="math/tex" id="MathJax-Element-13">x = -1e6</script>. Since <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-171" role="math" style="width: 2.871em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.359em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.437em 1002.31em 2.462em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-172"><span class="mi" id="MathJax-Span-173" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-174" style="font-family: MathJax_Main; padding-left: 0.259em;">&lt;</span><span class="mn" id="MathJax-Span-175" style="font-family: MathJax_Main; padding-left: 0.259em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>&lt;</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-14">x < 0</script>, the analytic gradient at this point is exactly zero. However, the numerical gradient would suddenly compute a non-zero gradient because <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-176" role="math" style="width: 4.511em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.691em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.59em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-177"><span class="mi" id="MathJax-Span-178" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-179" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-180" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-181" style="font-family: MathJax_Main; padding-left: 0.207em;">+</span><span class="mi" id="MathJax-Span-182" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-183" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-15">f(x+h)</script> might cross over the kink (e.g. if <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-16-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-184" role="math" style="width: 5.535em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.511em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.386em 1004.46em 2.462em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-185"><span class="mi" id="MathJax-Span-186" style="font-family: MathJax_Math-italic;">h</span><span class="mo" id="MathJax-Span-187" style="font-family: MathJax_Main; padding-left: 0.259em;">&gt;</span><span class="mn" id="MathJax-Span-188" style="font-family: MathJax_Main; padding-left: 0.259em;">1</span><span class="mi" id="MathJax-Span-189" style="font-family: MathJax_Math-italic;">e</span><span class="mo" id="MathJax-Span-190" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="mn" id="MathJax-Span-191" style="font-family: MathJax_Main; padding-left: 0.207em;">6</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mo>&gt;</mo><mn>1</mn><mi>e</mi><mo>−</mo><mn>6</mn></math></span></span><script type="math/tex" id="MathJax-Element-16">h > 1e-6</script>) and introduce a non-zero contribution. You might think that this is a pathological case, but in fact this case can be very common. For example, an SVM for CIFAR-10 contains up to 450,000 <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-17-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-192" role="math" style="width: 5.228em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.255em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1004.15em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-193"><span class="mi" id="MathJax-Span-194" style="font-family: MathJax_Math-italic;">m</span><span class="mi" id="MathJax-Span-195" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-196" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-197" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-198" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-199" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-200" style="font-family: MathJax_Math-italic; padding-left: 0.156em;">x</span><span class="mo" id="MathJax-Span-201" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-17">max(0,x)</script> terms because there are 50,000 examples and each example yields 9 terms to the objective. Moreover, a Neural Network with an SVM classifier will contain many more kinks due to ReLUs.</p>

<p>Note that it is possible to know if a kink was crossed in the evaluation of the loss. This can be done by keeping track of the identities of all “winners” in a function of form <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-18-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-202" role="math" style="width: 5.228em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.255em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1004.15em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-203"><span class="mi" id="MathJax-Span-204" style="font-family: MathJax_Math-italic;">m</span><span class="mi" id="MathJax-Span-205" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-206" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-207" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-208" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-209" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-210" style="font-family: MathJax_Math-italic; padding-left: 0.156em;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-211" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-18">max(x,y)</script>; That is, was x or y higher during the forward pass. If the identity of at least one winner changes when evaluating <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-19-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-212" role="math" style="width: 4.511em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.691em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.59em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-213"><span class="mi" id="MathJax-Span-214" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-215" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-216" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-217" style="font-family: MathJax_Main; padding-left: 0.207em;">+</span><span class="mi" id="MathJax-Span-218" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-219" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-19">f(x+h)</script> and then <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-20-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-220" role="math" style="width: 4.511em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.691em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.59em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-221"><span class="mi" id="MathJax-Span-222" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-223" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-224" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-225" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="mi" id="MathJax-Span-226" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-227" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-20">f(x-h)</script>, then a kink was crossed and the numerical gradient will not be exact.</p>

<p><strong>Use only few datapoints</strong>. One fix to the above problem of kinks is to use fewer datapoints, since loss functions that contain kinks (e.g. due to use of ReLUs or margin losses etc.) will have fewer kinks with fewer datapoints, so it is less likely for you to cross one when you perform the finite different approximation. Moreover, if your gradcheck for only ~2 or 3 datapoints then you would almost certainly gradcheck for an entire batch. Using very few datapoints also makes your gradient check faster and more efficient.</p>

<p><strong>Be careful with the step size h</strong>. It is not necessarily the case that smaller is better, because when <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-21-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-228" role="math" style="width: 0.72em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.566em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1000.57em 2.359em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-229"><span class="mi" id="MathJax-Span-230" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-21">h</script> is much smaller, you may start running into numerical precision problems. Sometimes when the gradient doesn’t check, it is possible that you change <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-22-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-231" role="math" style="width: 0.72em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.566em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1000.57em 2.359em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-232"><span class="mi" id="MathJax-Span-233" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-22">h</script> to be 1e-4 or 1e-6 and suddenly the gradient will be correct. This <a href="http://en.wikipedia.org/wiki/Numerical_differentiation">wikipedia article</a> contains a chart that plots the value of <strong>h</strong> on the x-axis and the numerical gradient error on the y-axis.</p>

<p><strong>Gradcheck during a “characteristic” mode of operation</strong>. It is important to realize that a gradient check is performed at a particular (and usually random), single point in the space of parameters. Even if the gradient check succeeds at that point, it is not immediately certain that the gradient is correctly implemented globally. Additionally, a random initialization might not be the most “characteristic” point in the space of parameters and may in fact introduce pathological situations where the gradient seems to be correctly implemented but isn’t. For instance, an SVM with very small weight initialization will assign almost exactly zero scores to all datapoints and the gradients will exhibit a particular pattern across all datapoints. An incorrect implementation of the gradient could still produce this pattern and not generalize to a more characteristic mode of operation where some scores are larger than others. Therefore, to be safe it is best to use a short <strong>burn-in</strong> time during which the network is allowed to learn and perform the gradient check after the loss starts to go down. The danger of performing it at the first iteration is that this could introduce pathological edge cases and mask an incorrect implementation of the gradient.</p>

<p><strong>Don’t let the regularization overwhelm the data</strong>. It is often the case that a loss function is a sum of the data loss and the regularization loss (e.g. L2 penalty on weights). One danger to be aware of is that the regularization loss may overwhelm the data loss, in which case the gradients will be primarily coming from the regularization term (which usually has a much simpler gradient expression). This can mask an incorrect implementation of the data loss gradient. Therefore, it is recommended to turn off regularization and check the data loss alone first, and then the regularization term second and independently. One way to perform the latter is to hack the code to remove the data loss contribution. Another way is to increase the regularization strength so as to ensure that its effect is non-negligible in the gradient check, and that an incorrect implementation would be spotted.</p>

<p><strong>Remember to turn off dropout/augmentations</strong>. When performing gradient check, remember to turn off any non-deterministic effects in the network, such as dropout, random data augmentations, etc. Otherwise these can clearly introduce huge errors when estimating the numerical gradient. The downside of turning off these effects is that you wouldn’t be gradient checking them (e.g. it might be that dropout isn’t backpropagated correctly). Therefore, a better solution might be to force a particular random seed before evaluating both <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-23-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-234" role="math" style="width: 4.511em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.691em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.59em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-235"><span class="mi" id="MathJax-Span-236" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-237" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-238" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-239" style="font-family: MathJax_Main; padding-left: 0.207em;">+</span><span class="mi" id="MathJax-Span-240" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-241" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-23">f(x+h)</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-24-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-242" role="math" style="width: 4.511em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.691em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.59em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-243"><span class="mi" id="MathJax-Span-244" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-245" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-246" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-247" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="mi" id="MathJax-Span-248" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">h</span><span class="mo" id="MathJax-Span-249" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-24">f(x-h)</script>, and when evaluating the analytic gradient.</p>

<p><strong>Check only few dimensions</strong>. In practice the gradients can have sizes of million parameters. In these cases it is only practical to check some of the dimensions of the gradient and assume that the others are correct. <strong>Be careful</strong>: One issue to be careful with is to make sure to gradient check a few dimensions for every separate parameter. In some applications, people combine the parameters into a single large parameter vector for convenience. In these cases, for example, the biases could only take up a tiny number of parameters from the whole vector, so it is important to not sample at random but to take this into account and check that all parameters receive the correct gradients.</p>

<p><a name="sanitycheck"></a></p>

<h3 id="before-learning-sanity-checks-tipstricks">Before learning: sanity checks Tips/Tricks</h3>

<p>Here are a few sanity checks you might consider running before you plunge into expensive optimization:</p>

<ul>
  <li><strong>Look for correct loss at chance performance.</strong> Make sure you’re getting the loss you expect when you initialize with small parameters. It’s best to first check the data loss alone (so set regularization strength to zero). For example, for CIFAR-10 with a Softmax classifier we would expect the initial loss to be 2.302, because we expect a diffuse probability of 0.1 for each class (since there are 10 classes), and Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302. For The Weston Watkins SVM, we expect all desired margins to be violated (since all scores are approximately zero), and hence expect a loss of 9 (since margin is 1 for each wrong class). If you’re not seeing these losses there might be issue with initialization.</li>
  <li>As a second sanity check, increasing the regularization strength should increase the loss</li>
  <li><strong>Overfit a tiny subset of data</strong>. Lastly and most importantly, before training on the full dataset try to train on a tiny portion (e.g. 20 examples) of your data and make sure you can achieve zero cost. For this experiment it’s also best to set regularization to zero, otherwise this can prevent you from getting zero cost. Unless you pass this sanity check with a small dataset it is not worth proceeding to the full dataset. Note that it may happen that you can overfit very small dataset but still have an incorrect implementation. For instance, if your datapoints’ features are random due to some bug, then it will be possible to overfit your small training set but you will never notice any generalization when you fold it your full dataset.</li>
</ul>

<p><a name="baby"></a></p>

<h3 id="babysitting-the-learning-process">Babysitting the learning process</h3>

<p>There are multiple useful quantities you should monitor during training of a neural network. These plots are the window into the training process and should be utilized to get intuitions about different hyperparameter settings and how they should be changed for more efficient learning.</p>

<p>The x-axis of the plots below are always in units of epochs, which measure how many times every example has been seen during training in expectation (e.g. one epoch means that every example has been seen once). It is preferable to track epochs rather than iterations since the number of iterations depends on the arbitrary setting of batch size.</p>

<p><a name="loss"></a></p>

<h4 id="loss-function">Loss function</h4>

<p>The first quantity that is useful to track during training is the loss, as it is evaluated on the individual batches during the forward pass. Below is a cartoon diagram showing the loss over time, and especially what the shape might tell you about the learning rate:</p>

<div class="fig figcenter fighighlight">
  <img src="./NN-3_files/learningrates.jpeg" width="49%">
  <img src="./NN-3_files/loss.jpeg" width="49%">
  <div class="figcaption">
    <b>Left:</b> A cartoon depicting the effects of different learning rates. With low learning rates the improvements will be linear. With high learning rates they will start to look more exponential. Higher learning rates will decay the loss faster, but they get stuck at worse values of loss (green line). This is because there is too much "energy" in the optimization and the parameters are bouncing around chaotically, unable to settle in a nice spot in the optimization landscape. <b>Right:</b> An example of a typical loss function over time, while training a small network on CIFAR-10 dataset. This loss function looks reasonable (it might indicate a slightly too small learning rate based on its speed of decay, but it's hard to say), and also indicates that the batch size might be a little too low (since the cost is a little too noisy).
  </div>
</div>

<p>The amount of “wiggle” in the loss is related to the batch size. When the batch size is 1, the wiggle will be relatively high. When the batch size is the full dataset, the wiggle will be minimal because every gradient update should be improving the loss function monotonically (unless the learning rate is set too high).</p>

<p>Some people prefer to plot their loss functions in the log domain. Since learning progress generally takes an exponential form shape, the plot appears more as a slightly more interpretable straight line, rather than a hockey stick. Additionally, if multiple cross-validated models are plotted on the same loss graph, the differences between them become more apparent.</p>

<p>Sometimes loss functions can look funny <a href="http://lossfunctions.tumblr.com/">lossfunctions.tumblr.com</a>.</p>

<p><a name="accuracy"></a></p>

<h4 id="trainval-accuracy">Train/Val accuracy</h4>

<p>The second important quantity to track while training a classifier is the validation/training accuracy. This plot can give you valuable insights into the amount of overfitting in your model:</p>

<div class="fig figleft fighighlight">
  <img src="./NN-3_files/accuracies.jpeg">
  <div class="figcaption">
    The gap between the training and validation accuracy indicates the amount of overfitting. Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it's possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well. This case indicates that your model capacity is not high enough: make the model larger by increasing the number of parameters.
  </div>
  <div style="clear:both"></div>
</div>

<p><a name="ratio"></a></p>

<h4 id="ratio-of-weightsupdates">Ratio of weights:updates</h4>

<p>The last quantity you might want to track is the ratio of the update magnitudes to the value magnitudes. Note: <em>updates</em>, not the raw gradients (e.g. in vanilla sgd this would be the gradient multiplied by the learning rate). You might want to evaluate and track this ratio for every set of parameters independently. A rough heuristic is that this ratio should be somewhere around 1e-3. If it is lower than this then the learning rate might be too low. If it is higher then the learning rate is likely too high. Here is a specific example:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># assume parameter vector W and its gradient vector dW</span>
<span class="n">param_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">update</span> <span class="o">=</span> <span class="o">-</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">dW</span> <span class="c"># simple SGD update</span>
<span class="n">update_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">update</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">W</span> <span class="o">+=</span> <span class="n">update</span> <span class="c"># the actual update</span>
<span class="k">print</span> <span class="n">update_scale</span> <span class="o">/</span> <span class="n">param_scale</span> <span class="c"># want ~1e-3</span>
</code></pre>
</div>

<p>Instead of tracking the min or the max, some people prefer to compute and track the norm of the gradients and their updates instead. These metrics are usually correlated and often give approximately the same results.</p>

<p><a name="distr"></a></p>

<h4 id="activation--gradient-distributions-per-layer">Activation / Gradient distributions per layer</h4>

<p>An incorrect initialization can slow down or even completely stall the learning process. Luckily, this issue can be diagnosed relatively easily. One way to do so is to plot activation/gradient histograms for all layers of the network. Intuitively, it is not a good sign to see any strange distributions - e.g. with tanh neurons we would like to see a distribution of neuron activations between the full range of [-1,1], instead of seeing all neurons outputting zero, or all neurons being completely saturated at either -1 or 1.</p>

<p><a name="vis"></a></p>

<h4 id="first-layer-visualizations">First-layer Visualizations</h4>

<p>Lastly, when one is working with image pixels it can be helpful and satisfying to plot the first-layer features visually:</p>

<div class="fig figcenter fighighlight">
  <img src="./NN-3_files/weights.jpeg" width="43%" style="margin-right:10px;">
  <img src="./NN-3_files/cnnweights.jpg" width="49%">
  <div class="figcaption">
    Examples of visualized weights for the first layer of a neural network. <b>Left</b>: Noisy features indicate could be a symptom: Unconverged network, improperly set learning rate, very low weight regularization penalty. <b>Right:</b> Nice, smooth, clean and diverse features are a good indication that the training is proceeding well.
  </div>
</div>

<p><a name="update"></a></p>

<h3 id="parameter-updates">Parameter updates</h3>

<p>Once the analytic gradient is computed with backpropagation, the gradients are used to perform a parameter update. There are several approaches for performing the update, which we discuss next.</p>

<p>We note that optimization for deep networks is currently a very active area of research. In this section we highlight some established and common techniques you may see in practice, briefly describe their intuition, but leave a detailed analysis outside of the scope of the class. We provide some further pointers for an interested reader.</p>

<p><a name="sgd"></a></p>

<h4 id="sgd-and-bells-and-whistles">SGD and bells and whistles</h4>

<p><strong>Vanilla update</strong>. The simplest form of update is to change the parameters along the negative gradient direction (since the gradient indicates the direction of increase, but we usually wish to minimize a loss function). Assuming a vector of parameters <code class="highlighter-rouge">x</code> and the gradient <code class="highlighter-rouge">dx</code>, the simplest update has the form:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Vanilla update</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span>
</code></pre>
</div>

<p>where <code class="highlighter-rouge">learning_rate</code> is a hyperparameter - a fixed constant. When evaluated on the full dataset, and when the learning rate is low enough, this is guaranteed to make non-negative progress on the loss function.</p>

<p><strong>Momentum update</strong> is another approach that almost always enjoys better converge rates on deep networks. This update can be motivated from a physical perspective of the optimization problem. In particular, the loss can be interpreted as a the height of a hilly terrain (and therefore also to the potential energy since <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-25-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-250" role="math" style="width: 4.869em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.998em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.386em 1004em 2.615em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-251"><span class="mi" id="MathJax-Span-252" style="font-family: MathJax_Math-italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mo" id="MathJax-Span-253" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="mi" id="MathJax-Span-254" style="font-family: MathJax_Math-italic; padding-left: 0.259em;">m</span><span class="mi" id="MathJax-Span-255" style="font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-256" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>U</mi><mo>=</mo><mi>m</mi><mi>g</mi><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-25">U = mgh</script> and therefore <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-26-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mo&gt;&amp;#x221D;&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-257" role="math" style="width: 3.23em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.615em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.386em 1002.62em 2.41em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-258"><span class="mi" id="MathJax-Span-259" style="font-family: MathJax_Math-italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mo" id="MathJax-Span-260" style="font-family: MathJax_Main; padding-left: 0.259em;">∝</span><span class="mi" id="MathJax-Span-261" style="font-family: MathJax_Math-italic; padding-left: 0.259em;">h</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>U</mi><mo>∝</mo><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-26"> U \propto h </script> ). Initializing the parameters with random numbers is equivalent to setting a particle with zero initial velocity at some location. The optimization process can then be seen as equivalent to the process of simulating the parameter vector (i.e. a particle) as rolling on the landscape.</p>

<p>Since the force on the particle is related to the gradient of potential energy (i.e. <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-27-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-262" role="math" style="width: 5.382em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.408em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.437em 1004.41em 2.462em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-263"><span class="mi" id="MathJax-Span-264" style="font-family: MathJax_Math-italic;">F<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mo" id="MathJax-Span-265" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="mo" id="MathJax-Span-266" style="font-family: MathJax_Main; padding-left: 0.259em;">−</span><span class="mi" id="MathJax-Span-267" style="font-family: MathJax_Main;">∇</span><span class="mi" id="MathJax-Span-268" style="font-family: MathJax_Math-italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mo>=</mo><mo>−</mo><mi mathvariant="normal">∇</mi><mi>U</mi></math></span></span><script type="math/tex" id="MathJax-Element-27">F = - \nabla U </script> ), the <strong>force</strong> felt by the particle is precisely the (negative) <strong>gradient</strong> of the loss function. Moreover, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-28-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-269" role="math" style="width: 4.203em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.435em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.437em 1003.43em 2.41em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-270"><span class="mi" id="MathJax-Span-271" style="font-family: MathJax_Math-italic;">F<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mo" id="MathJax-Span-272" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="mi" id="MathJax-Span-273" style="font-family: MathJax_Math-italic; padding-left: 0.259em;">m</span><span class="mi" id="MathJax-Span-274" style="font-family: MathJax_Math-italic;">a</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mo>=</mo><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-28">F = ma </script> so the (negative) gradient is in this view proportional to the acceleration of the particle. Note that this is different from the SGD update shown above, where the gradient directly integrates the position. Instead, the physics view suggests an update in which the gradient only directly influences the velocity, which in turn has an effect on the position:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Momentum update</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="c"># integrate velocity</span>
<span class="n">x</span> <span class="o">+=</span> <span class="n">v</span> <span class="c"># integrate position</span>
</code></pre>
</div>

<p>Here we see an introduction of a <code class="highlighter-rouge">v</code> variable that is initialized at zero, and an additional hyperparameter (<code class="highlighter-rouge">mu</code>). As an unfortunate misnomer, this variable is in optimization referred to as <em>momentum</em> (its typical value is about 0.9), but its physical meaning is more consistent with the coefficient of friction. Effectively, this variable damps the velocity and reduces the kinetic energy of the system, or otherwise the particle would never come to a stop at the bottom of a hill. When cross-validated, this parameter is usually set to values such as [0.5, 0.9, 0.95, 0.99]. Similar to annealing schedules for learning rates (discussed later, below), optimization can sometimes benefit a little from momentum schedules, where the momentum is increased in later stages of learning. A typical setting is to start with momentum of about 0.5 and anneal it to 0.99 or so over multiple epochs.</p>

<blockquote>
  <p>With Momentum update, the parameter vector will build up velocity in any direction that has consistent gradient.</p>
</blockquote>

<p><strong>Nesterov Momentum</strong> is a slightly different version of the momentum update has recently been gaining popularity. It enjoys stronger theoretical converge guarantees for convex functions and in practice it also consistenly works slightly better than standard momentum.</p>

<p>The core idea behind Nesterov momentum is that when the current parameter vector is at some position <code class="highlighter-rouge">x</code>, then looking at the momentum update above, we know that the momentum term alone (i.e. ignoring the second term with the gradient) is about to nudge the parameter vector by <code class="highlighter-rouge">mu * v</code>. Therefore, if we are about to compute the gradient, we can treat the future approximate position <code class="highlighter-rouge">x + mu * v</code> as a “lookahead” - this is a point in the vicinity of where we are soon going to end up. Hence, it makes sense to compute the gradient at <code class="highlighter-rouge">x + mu * v</code> instead of at the “old/stale” position <code class="highlighter-rouge">x</code>.</p>

<div class="fig figcenter fighighlight">
  <img src="./NN-3_files/nesterov.jpeg">
  <div class="figcaption">
    Nesterov momentum. Instead of evaluating gradient at the current position (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum we therefore instead evaluate the gradient at this "looked-ahead" position.
  </div>
</div>

<p>That is, in a slightly awkward notation, we would like to do the following:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x_ahead</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">v</span>
<span class="c"># evaluate dx_ahead (the gradient at x_ahead instead of at x)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx_ahead</span>
<span class="n">x</span> <span class="o">+=</span> <span class="n">v</span>
</code></pre>
</div>

<p>However, in practice people prefer to express the update to look as similar to vanilla SGD or to the previous momentum update as possible. This is possible to achieve by manipulating the update above with a variable transform <code class="highlighter-rouge">x_ahead = x + mu * v</code>, and then expressing the update in terms of <code class="highlighter-rouge">x_ahead</code> instead of <code class="highlighter-rouge">x</code>. That is, the parameter vector we are actually storing is always the ahead version. The equations in terms of <code class="highlighter-rouge">x_ahead</code> (but renaming it back to <code class="highlighter-rouge">x</code>) then become:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">v_prev</span> <span class="o">=</span> <span class="n">v</span> <span class="c"># back this up</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="c"># velocity update stays the same</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span><span class="n">mu</span> <span class="o">*</span> <span class="n">v_prev</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">v</span> <span class="c"># position update changes form</span>
</code></pre>
</div>

<p>We recommend this further reading to understand the source of these equations and the mathematical formulation of Nesterov’s Accelerated Momentum (NAG):</p>

<ul>
  <li><a href="http://arxiv.org/pdf/1212.0901v2.pdf">Advances in optimizing Recurrent Networks</a> by Yoshua Bengio, Section 3.5.</li>
  <li><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf">Ilya Sutskever’s thesis</a> (pdf) contains a longer exposition of the topic in section 7.2</li>
</ul>

<p><a name="anneal"></a></p>

<h4 id="annealing-the-learning-rate">Annealing the learning rate</h4>

<p>In training deep networks, it is usually helpful to anneal the learning rate over time. Good intuition to have in mind is that with a high learning rate, the system contains too much kinetic energy and the parameter vector bounces around chaotically, unable to settle down into deeper, but narrower parts of the loss function. Knowing when to decay the learning rate can be tricky: Decay it slowly and you’ll be wasting computation bouncing around chaotically with little improvement for a long time. But decay it too aggressively and the system will cool too quickly, unable to reach the best position it can. There are three common types of implementing the learning rate decay:</p>

<ul>
  <li><strong>Step decay</strong>: Reduce the learning rate by some factor every few epochs. Typical values might be reducing the learning rate by a half every 5 epochs, or by 0.1 every 20 epochs. These numbers depend heavily on the type of problem and the model. One heuristic you may see in practice is to watch the validation error while training with a fixed learning rate, and reduce the learning rate by a constant (e.g. 0.5) whenever the validation error stops improving.</li>
  <li><strong>Exponential decay.</strong> has the mathematical form <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-29-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-275" role="math" style="width: 5.689em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.664em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.232em 1004.66em 2.564em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-276"><span class="mi" id="MathJax-Span-277" style="font-family: MathJax_Math-italic;">α</span><span class="mo" id="MathJax-Span-278" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="msubsup" id="MathJax-Span-279" style="padding-left: 0.259em;"><span style="display: inline-block; position: relative; width: 1.078em; height: 0px;"><span style="position: absolute; clip: rect(3.384em 1000.62em 4.152em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-280" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.617em;"><span class="mn" id="MathJax-Span-281" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-282"><span style="display: inline-block; position: relative; width: 1.693em; height: 0px;"><span style="position: absolute; clip: rect(3.384em 1000.41em 4.152em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-283" style="font-family: MathJax_Math-italic;">e</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -4.352em; left: 0.464em;"><span class="texatom" id="MathJax-Span-284"><span class="mrow" id="MathJax-Span-285"><span class="mo" id="MathJax-Span-286" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-287" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mi" id="MathJax-Span-288" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><msup><mi>e</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mi>k</mi><mi>t</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-29">\alpha = \alpha_0 e^{-k t}</script>, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-30-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-289" role="math" style="width: 2.462em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.001em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.386em 1002em 2.615em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-290"><span class="msubsup" id="MathJax-Span-291"><span style="display: inline-block; position: relative; width: 1.078em; height: 0px;"><span style="position: absolute; clip: rect(3.384em 1000.62em 4.152em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-292" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.617em;"><span class="mn" id="MathJax-Span-293" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-294" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-295" style="font-family: MathJax_Math-italic; padding-left: 0.156em;">k</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>α</mi><mn>0</mn></msub><mo>,</mo><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-30">\alpha_0, k</script> are hyperparameters and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-31-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-296" role="math" style="width: 0.464em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.361em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.437em 1000.31em 2.359em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-297"><span class="mi" id="MathJax-Span-298" style="font-family: MathJax_Math-italic;">t</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi></math></span></span><script type="math/tex" id="MathJax-Element-31">t</script> is the iteration number (but you can also use units of epochs).</li>
  <li><strong>1/t decay</strong> has the mathematical form <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-32-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-299" role="math" style="width: 8.353em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.816em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1006.71em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-300"><span class="mi" id="MathJax-Span-301" style="font-family: MathJax_Math-italic;">α</span><span class="mo" id="MathJax-Span-302" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="msubsup" id="MathJax-Span-303" style="padding-left: 0.259em;"><span style="display: inline-block; position: relative; width: 1.078em; height: 0px;"><span style="position: absolute; clip: rect(3.384em 1000.62em 4.152em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-304" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.617em;"><span class="mn" id="MathJax-Span-305" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="texatom" id="MathJax-Span-306"><span class="mrow" id="MathJax-Span-307"><span class="mo" id="MathJax-Span-308" style="font-family: MathJax_Main;">/</span></span></span><span class="mo" id="MathJax-Span-309" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-310" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-311" style="font-family: MathJax_Main; padding-left: 0.207em;">+</span><span class="mi" id="MathJax-Span-312" style="font-family: MathJax_Math-italic; padding-left: 0.207em;">k</span><span class="mi" id="MathJax-Span-313" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-314" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>k</mi><mi>t</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-32">\alpha = \alpha_0 / (1 + k t )</script> where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-33-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-315" role="math" style="width: 2.308em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.898em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.386em 1001.9em 2.615em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-316"><span class="msubsup" id="MathJax-Span-317"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(3.384em 1000.51em 4.152em -999.997em); top: -3.993em; left: 0.003em;"><span class="mi" id="MathJax-Span-318" style="font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.515em;"><span class="mn" id="MathJax-Span-319" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-320" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-321" style="font-family: MathJax_Math-italic; padding-left: 0.156em;">k</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-33">a_0, k</script> are hyperparameters and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-34-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-322" role="math" style="width: 0.464em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.361em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.437em 1000.31em 2.359em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-323"><span class="mi" id="MathJax-Span-324" style="font-family: MathJax_Math-italic;">t</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi></math></span></span><script type="math/tex" id="MathJax-Element-34">t</script> is the iteration number.</li>
</ul>

<p>In practice, we find that the step decay dropout is slightly preferable because the hyperparameters it involves (the fraction of decay and the step timings in units of epochs) are more interpretable than the hyperparameter <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-35-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-325" role="math" style="width: 0.72em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.566em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1000.57em 2.359em -999.997em); top: -2.2em; left: 0.003em;"><span class="mrow" id="MathJax-Span-326"><span class="mi" id="MathJax-Span-327" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-35">k</script>. Lastly, if you can afford the computational budget, err on the side of slower decay and train for a longer time.</p>

<p><a name="second"></a></p>

<h4 id="second-order-methods">Second order methods</h4>

<p>A second, popular group of methods for optimization in context of deep learning is based on <a href="http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s method</a>, which iterates the following update:</p>

<span class="MathJax_Preview" style="color: inherit;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-36-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-328" role="math" style="width: 13.322em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.914em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.232em 1010.81em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-329"><span class="mi" id="MathJax-Span-330" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-331" style="font-family: MathJax_Main; padding-left: 0.259em;">←</span><span class="mi" id="MathJax-Span-332" style="font-family: MathJax_Math-italic; padding-left: 0.259em;">x</span><span class="mo" id="MathJax-Span-333" style="font-family: MathJax_Main; padding-left: 0.207em;">−</span><span class="mo" id="MathJax-Span-334" style="font-family: MathJax_Main; padding-left: 0.207em;">[</span><span class="mi" id="MathJax-Span-335" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-336" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-337" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-338" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-339" style="font-family: MathJax_Main;">)</span><span class="msubsup" id="MathJax-Span-340"><span style="display: inline-block; position: relative; width: 1.283em; height: 0px;"><span style="position: absolute; clip: rect(3.076em 1000.16em 4.408em -999.997em); top: -3.993em; left: 0.003em;"><span class="mo" id="MathJax-Span-341" style="font-family: MathJax_Main;">]</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -4.403em; left: 0.259em;"><span class="texatom" id="MathJax-Span-342"><span class="mrow" id="MathJax-Span-343"><span class="mo" id="MathJax-Span-344" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-345" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mi" id="MathJax-Span-346" style="font-family: MathJax_Main;">∇</span><span class="mi" id="MathJax-Span-347" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-348" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-349" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-350" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>x</mi><mo stretchy="false">←</mo><mi>x</mi><mo>−</mo><mo stretchy="false">[</mo><mi>H</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">]</mo><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="normal">∇</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-36">x \leftarrow x - [H f(x)]^{-1} \nabla f(x)</script>

<p>Here, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-37-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-351" role="math" style="width: 3.435em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.82em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1002.72em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-352"><span class="mi" id="MathJax-Span-353" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-354" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-355" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-356" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-357" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-37">H f(x)</script> is the <a href="http://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a>, which is a square matrix of second-order partial derivatives of the function. The term <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-38-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-358" role="math" style="width: 3.332em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.718em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1002.62em 2.666em -999.997em); top: -2.252em; left: 0.003em;"><span class="mrow" id="MathJax-Span-359"><span class="mi" id="MathJax-Span-360" style="font-family: MathJax_Main;">∇</span><span class="mi" id="MathJax-Span-361" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-362" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-363" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-364" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.257em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">∇</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-38">\nabla f(x)</script> is the gradient vector, as seen in Gradient Descent. Intuitively, the Hessian describes the local curvature of the loss function, which allows us to perform a more efficient update. In particular, multiplying by the inverse Hessian leads the optimization to take more aggressive steps in directions of shallow curvature and shorter steps in directions of steep curvature. Note, crucially, the absence of any learning rate hyperparameters in the update formula, which the proponents of these methods cite this as a large advantage over first-order methods.</p>

<p>However, the update above is impractical for most deep learning applications because computing (and inverting) the Hessian in its explicit form is a very costly process in both space and time. For instance, a Neural Network with one million parameters would have a Hessian matrix of size [1,000,000 x 1,000,000], occupying approximately 3725 gigabytes of RAM. Hence, a large variety of <em>quasi-Newton</em> methods have been developed that seek to approximate the inverse Hessian. Among these, the most popular is <a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a>, which uses the information in the gradients over time to form the approximation implicitly (i.e. the full matrix is never computed).</p>

<p>However, even after we eliminate the memory concerns, a large downside of a naive application of L-BFGS is that it must be computed over the entire training set, which could contain millions of examples. Unlike mini-batch SGD, getting L-BFGS to work on mini-batches is more tricky and an active area of research.</p>

<p><strong>In practice</strong>, it is currently not common to see L-BFGS or similar second-order methods applied to large-scale Deep Learning and Convolutional Neural Networks. Instead, SGD variants based on (Nesterov’s) momentum are more standard because they are simpler and scale more easily.</p>

<p>Additional references:</p>

<ul>
  <li><a href="http://research.google.com/archive/large_deep_networks_nips2012.html">Large Scale Distributed Deep Networks</a> is a paper from the Google Brain team, comparing L-BFGS and SGD variants in large-scale distributed optimization.</li>
  <li><a href="http://arxiv.org/abs/1311.2115">SFO</a> algorithm strives to combine the advantages of SGD with advantages of L-BFGS.</li>
</ul>

<p><a name="ada"></a></p>

<h4 id="per-parameter-adaptive-learning-rate-methods">Per-parameter adaptive learning rate methods</h4>

<p>All previous approaches we’ve discussed so far manipulated the learning rate globally and equally for all parameters. Tuning the learning rates is an expensive process, so much work has gone into devising methods that can adaptively tune the learning rates, and even do so per parameter. Many of these methods may still require other hyperparameter settings, but the argument is that they are well-behaved for a broader range of hyperparameter values than the raw learning rate. In this section we highlight some common adaptive methods you may encounter in practice:</p>

<p><strong>Adagrad</strong> is an adaptive learning rate method originally proposed by <a href="http://jmlr.org/papers/v12/duchi11a.html">Duchi et al.</a>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Assume the gradient dx and parameter vector x</span>
<span class="n">cache</span> <span class="o">+=</span> <span class="n">dx</span><span class="o">**</span><span class="mi">2</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</code></pre>
</div>

<p>Notice that the variable <code class="highlighter-rouge">cache</code> has size equal to the size of the gradient, and keeps track of per-parameter sum of squared gradients. This is then used to normalize the parameter update step, element-wise. Notice that the weights that receive high gradients will have their effective learning rate reduced, while weights that receive small or infrequent updates will have their effective learning rate increased. Amusingly, the square root operation turns out to be very important and without it the algorithm performs much worse. The smoothing term <code class="highlighter-rouge">eps</code> (usually set somewhere in range from 1e-4 to 1e-8) avoids division by zero. A downside of Adagrad is that in case of Deep Learning, the monotonic learning rate usually proves too aggressive and stops learning too early.</p>

<p><strong>RMSprop.</strong> RMSprop is a very effective, but currently unpublished adaptive learning rate method. Amusingly, everyone who uses this method in their work currently cites <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">slide 29 of Lecture 6</a> of Geoff Hinton’s Coursera class. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. In particular, it uses a moving average of squared gradients instead, giving:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">cache</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">cache</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span><span class="o">**</span><span class="mi">2</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</code></pre>
</div>

<p>Here, <code class="highlighter-rouge">decay_rate</code> is a hyperparameter and typical values are [0.9, 0.99, 0.999]. Notice that the <code class="highlighter-rouge">x+=</code> update is identical to Adagrad, but the <code class="highlighter-rouge">cache</code> variable is a “leaky”. Hence, RMSProp still modulates the learning rate of each weight based on the magnitudes of its gradients, which has a beneficial equalizing effect, but unlike Adagrad the updates do not get monotonically smaller.</p>

<p><strong>Adam.</strong> <a href="http://arxiv.org/abs/1412.6980">Adam</a> is a recently proposed update that looks a bit like RMSProp with momentum. The (simplified) update looks as follows:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">m</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">dx</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">beta2</span><span class="o">*</span><span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dx</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</code></pre>
</div>

<p>Notice that the update looks exactly as RMSProp update, except the “smooth” version of the gradient <code class="highlighter-rouge">m</code> is used instead of the raw (and perhaps noisy) gradient vector <code class="highlighter-rouge">dx</code>. Recommended values in the paper are <code class="highlighter-rouge">eps = 1e-8</code>, <code class="highlighter-rouge">beta1 = 0.9</code>, <code class="highlighter-rouge">beta2 = 0.999</code>. In practice Adam is currently recommended as the default algorithm to use, and often works slightly better than RMSProp. However, it is often also worth trying SGD+Nesterov Momentum as an alternative. The full Adam update also includes a <em>bias correction</em> mechanism, which compensates for the fact that in the first few time steps the vectors <code class="highlighter-rouge">m,v</code> are both initialized and therefore biased at zero, before they fully “warm up”. We refer the reader to the paper for the details, or the course slides where this is expanded on.</p>

<p>Additional References:</p>

<ul>
  <li><a href="http://arxiv.org/abs/1312.6055">Unit Tests for Stochastic Optimization</a> proposes a series of tests as a standardized benchmark for stochastic optimization.</li>
</ul>

<div class="fig figcenter fighighlight">
  <img src="./NN-3_files/opt2.gif" width="49%" style="margin-right:10px;">
  <img src="./NN-3_files/opt1.gif" width="49%">
  <div class="figcaption">
    Animations that may help your intuitions about the learning process dynamics. <b>Left:</b> Contours of a loss surface and time evolution of different optimization algorithms. Notice the "overshooting" behavior of momentum-based methods, which make the optimization look like a ball rolling down the hill. <b>Right:</b> A visualization of a saddle point in the optimization landscape, where the curvature along different dimension has different signs (one dimension curves up and another down). Notice that SGD has a very hard time breaking symmetry and gets stuck on the top. Conversely, algorithms such as RMSprop will see very low gradients in the saddle direction. Due to the denominator term in the RMSprop update, this will increase the effective learning rate along this direction, helping RMSProp proceed. Images credit: <a href="https://twitter.com/alecrad">Alec Radford</a>.
  </div>
</div>

<p><a name="hyper"></a></p>

<h3 id="hyperparameter-optimization">Hyperparameter optimization</h3>

<p>As we’ve seen, training Neural Networks can involve many hyperparameter settings. The most common hyperparameters in context of Neural Networks include:</p>

<ul>
  <li>the initial learning rate</li>
  <li>learning rate decay schedule (such as the decay constant)</li>
  <li>regularization strength (L2 penalty, dropout strength)</li>
</ul>

<p>But as saw, there are many more relatively less sensitive hyperparameters, for example in per-parameter adaptive learning methods, the setting of momentum and its schedule, etc. In this section we describe some additional tips and tricks for performing the hyperparameter search:</p>

<p><strong>Implementation</strong>. Larger Neural Networks typically require a long time to train, so performing hyperparameter search can take many days/weeks. It is important to keep this in mind since it influences the design of your code base. One particular design is to have a <strong>worker</strong> that continuously samples random hyperparameters and performs the optimization. During the training, the worker will keep track of the validation performance after every epoch, and writes a model checkpoint (together with miscellaneous training statistics such as the loss over time) to a file, preferably on a shared file system. It is useful to include the validation performance directly in the filename, so that it is simple to inspect and sort the progress. Then there is a second program which we will call a <strong>master</strong>, which launches or kills workers across a computing cluster, and may additionally inspect the checkpoints written by workers and plot their training statistics, etc.</p>

<p><strong>Prefer one validation fold to cross-validation</strong>. In most cases a single validation set of respectable size substantially simplifies the code base, without the need for cross-validation with multiple folds. You’ll hear people say they “cross-validated” a parameter, but many times it is assumed that they still only used a single validation set.</p>

<p><strong>Hyperparameter ranges</strong>. Search for hyperparameters on log scale. For example, a typical sampling of the learning rate would look as follows: <code class="highlighter-rouge">learning_rate = 10 ** uniform(-6, 1)</code>. That is, we are generating a random number from a uniform distribution, but then raising it to the power of 10. The same strategy should be used for the regularization strength. Intuitively, this is because learning rate and regularization strength have multiplicative effects on the training dynamics. For example, a fixed change of adding 0.01 to a learning rate has huge effects on the dynamics if the learning rate is 0.001, but nearly no effect if the learning rate when it is 10. This is because the learning rate multiplies the computed gradient in the update. Therefore, it is much more natural to consider a range of learning rate multiplied or divided by some value, than a range of learning rate added or subtracted to by some value. Some parameters (e.g. dropout) are instead usually searched in the original scale (e.g. <code class="highlighter-rouge">dropout = uniform(0,1)</code>).</p>

<p><strong>Prefer random search to grid search</strong>. As argued by Bergstra and Bengio in <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a>, “randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid”. As it turns out, this is also usually easier to implement.</p>

<div class="fig figcenter fighighlight">
  <img src="./NN-3_files/gridsearchbad.jpeg" width="50%">
  <div class="figcaption">
    Core illustration from <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a> by Bergstra and Bengio. It is very often the case that some of the hyperparameters matter much more than others (e.g. top hyperparam vs. left one in this figure). Performing random search rather than grid search allows you to much more precisely discover good values for the important ones.
  </div>
</div>

<p><strong>Careful with best values on border</strong>. Sometimes it can happen that you’re searching for a hyperparameter (e.g. learning rate) in a bad range. For example, suppose we use <code class="highlighter-rouge">learning_rate = 10 ** uniform(-6, 1)</code>. Once we receive the results, it is important to double check that the final learning rate is not at the edge of this interval, or otherwise you may be missing more optimal hyperparameter setting beyond the interval.</p>

<p><strong>Stage your search from coarse to fine</strong>. In practice, it can be helpful to first search in coarse ranges (e.g. 10 ** [-6, 1]), and then depending on where the best results are turning up, narrow the range. Also, it can be helpful to perform the initial coarse search while only training for 1 epoch or even less, because many hyperparameter settings can lead the model to not learn at all, or immediately explode with infinite cost. The second stage could then perform a narrower search with 5 epochs, and the last stage could perform a detailed search in the final range for many more epochs (for example).</p>

<p><strong>Bayesian Hyperparameter Optimization</strong> is a whole area of research devoted to coming up with algorithms that try to more efficiently navigate the space of hyperparameters. The core idea is to appropriately balance the exploration - exploitation trade-off when querying the performance at different hyperparameters. Multiple libraries have been developed based on these models as well, among some of the better known ones are <a href="https://github.com/JasperSnoek/spearmint">Spearmint</a>, <a href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/">SMAC</a>, and <a href="http://jaberg.github.io/hyperopt/">Hyperopt</a>. However, in practical settings with ConvNets it is still relatively difficult to beat random search in a carefully-chosen intervals. See some additional from-the-trenches discussion <a href="http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html">here</a>.</p>

<p><a name="eval"></a></p>

<h2 id="evaluation">Evaluation</h2>

<p><a name="ensemble"></a></p>

<h3 id="model-ensembles">Model Ensembles</h3>

<p>In practice, one reliable approach to improving the performance of Neural Networks by a few percent is to train multiple independent models, and at test time average their predictions. As the number of models in the ensemble increases, the performance typically monotonically improves (though with diminishing returns). Moreover, the improvements are more dramatic with higher model variety in the ensemble. There are a few approaches to forming an ensemble:</p>

<ul>
  <li><strong>Same model, different initializations</strong>. Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization.</li>
  <li><strong>Top models discovered during cross-validation</strong>. Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesn’t require additional retraining of models after cross-validation</li>
  <li><strong>Different checkpoints of a single model</strong>. If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap.</li>
  <li><strong>Running average of parameters during training</strong>. Related to the last point, a cheap way of almost always getting an extra percent or two of performance is to maintain a second copy of the network’s weights in memory that maintains an exponentially decaying sum of previous weights during training. This way you’re averaging the state of the network over last several iterations. You will find that this “smoothed” version of the weights over last few steps almost always achieves better validation error. The rough intuition to have in mind is that the objective is bowl-shaped and your network is jumping around the mode, so the average has a higher chance of being somewhere nearer the mode.</li>
</ul>

<p>One disadvantage of model ensembles is that they take longer to evaluate on test example. An interested reader may find the recent work from Geoff Hinton on <a href="https://www.youtube.com/watch?v=EK61htlw8hY">“Dark Knowledge”</a> inspiring, where the idea is to “distill” a good ensemble back to a single model by incorporating the ensemble log likelihoods into a modified objective.</p>

<p><a name="summary"></a></p>

<h2 id="summary">Summary</h2>

<p>To train a Neural Network:</p>

<ul>
  <li>Gradient check your implementation with a small batch of data and be aware of the pitfalls.</li>
  <li>As a sanity check, make sure your initial loss is reasonable, and that you can achieve 100% training accuracy on a very small portion of the data</li>
  <li>During training, monitor the loss, the training/validation accuracy, and if you’re feeling fancier, the magnitude of updates in relation to parameter values (it should be ~1e-3), and when dealing with ConvNets, the first-layer weights.</li>
  <li>The two recommended updates to use are either SGD+Nesterov Momentum or Adam.</li>
  <li>Decay your learning rate over the period of the training. For example, halve the learning rate after a fixed number of epochs, or whenever the validation accuracy tops off.</li>
  <li>Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs)</li>
  <li>Form model ensembles for extra performance</li>
</ul>

<p><a name="add"></a></p>

<h2 id="additional-references">Additional References</h2>

<ul>
  <li><a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">SGD</a> tips and tricks from Leon Bottou</li>
  <li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient BackProp</a> (pdf) from Yann LeCun</li>
  <li><a href="http://arxiv.org/pdf/1206.5533v2.pdf">Practical Recommendations for Gradient-Based Training of Deep
Architectures</a> from Yoshua Bengio</li>
</ul>

  </article>

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>
        
        <li>
          <a href="https://github.com/cs231n">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
              </svg>
            </span>
            <span class="username">cs231n</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/cs231n">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
              </svg>
            </span>
            <span class="username">cs231n</span>
          </a>
        </li>
        <li>
          <a href="mailto:karpathy@cs.stanford.edu">karpathy@cs.stanford.edu</a>
        </li>
      </ul>
    </div>

    <div class="footer-col-2 column">
        
    </div>

    <div class="footer-col-3 column">
      
    </div>

  </div>

</footer>


    <!-- mathjax -->
    <script type="text/javascript" src="./NN-3_files/MathJax.js"></script>
    
<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, Times;"></div></div></body></html>